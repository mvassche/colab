{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbGPhTLYrRayTnPkNIevlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvassche/colab/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoI1yC8hVFiA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "## parameters ##\n",
        "\n",
        "# Set GPU according to availability\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\" # \"0,1,2,3\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the mode (train or inference)\n",
        "mode = \"inference\" # options: \"train\", \"inference\" (of \"trial\" if you want to check some things without training/doing inference)\n",
        "\n",
        "# If inference, set path to saved model, select inference mode (sentence or dataset) and give inference input (sentence or path to dataset)\n",
        "if mode == \"inference\":\n",
        "  inference_modelpath = \"my_model2/checkpoint-128\"\n",
        "  inference_mode = \"dataset\" # options: \"sentence\", \"dataset\"\n",
        "  inference_input = \"Wow.. Ik ben echt heel bang. Nooit gedacht dat dit nog zou gebeuren.\"\n",
        "  if inference_mode == \"dataset\":\n",
        "    inference_input = \"/home/luna/Demo/multi_cat_inference.txt\"\n",
        "    # Make sure to create a folder called 'predictions'\n",
        "    #pred_path = \"predictions/my_model2_checkpoint-128.txt\" # enter prediction path manually\n",
        "    pred_path = \"predictions/\" + inference_modelpath.split('/')[0] + \"_\" + inference_modelpath.split('/')[1] + \".txt\" # or create file name based on model name\n",
        "\n",
        "# Data file should be in tsv-format with in the first column the id, second column the text, and the third column the label.\n",
        "# First line should be the column names: id\\ttext\\tlabel\n",
        "# For multi-class single-label problems: labels should be converted to numbers from 0 to n\n",
        "# For multi-class multi-label problems: labels should be converted to numbers from 0 to n and be separated by commas (e.g. 0,3)\n",
        "# For regression problems: a value is given (either an integer or a float with a point as decimal separator)\n",
        "\n",
        "data_paths = {\"train\": \"/home/luna/transformers3_8/Demo/multi_cat_train.txt\", \"eval\": \"/home/luna/transformers3_8/Demo/multi_cat_train.txt\"}\n",
        "if mode == \"inference\" and inference_mode == \"dataset\":\n",
        "  data_paths[\"inference\"] = inference_input\n",
        "\n",
        "# Specify the task: \"single_label_classification\", \"multi_label_classification\" or \"regression\"\n",
        "task = \"single_label_classification\"\n",
        "\n",
        "# Provide mapping for labels and numbers\n",
        "if task != \"regression\":\n",
        "  id2label = {0: \"neutral\", 1: \"anger\", 2: \"fear\", 3: \"joy\", 4: \"love\", 5: \"sadness\"}\n",
        "  label2id = {\"neutral\": 0, \"anger\": 1, \"fear\": 2, \"joy\": 3, \"love\": 4, \"sadness\": 5}\n",
        "else:\n",
        "  id2label = None\n",
        "  label2id = None\n",
        "\n",
        "\n",
        "model_config = {\n",
        "    \"model_weights\": \"pdelobelle/robbert-v2-dutch-base\", # check huggingface\n",
        "    \"num_labels\": 6, # for single-label and multi-label classification: num_labels = n; for regression: num_labels = 1\n",
        "    \"max_length\": 128,\n",
        "    \"device\": device\n",
        "}\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_model2\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\", # set to \"no\" if you don't want to evaluate during training.\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1 # Only keeps last checkpoint, deletes the older checkpoints. Comment out if you want to keep all checkpoints.\n",
        "    #push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# huggingface libraries\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "\n",
        "# import options\n",
        "from options import *\n",
        "\n",
        "\n",
        "## Load and preprocess dataset ##\n",
        "\n",
        "# Choose tokenizer (parameter: model_weights)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_config[\"model_weights\"])\n",
        "\n",
        "# Load dataset with load_dataset() from huggingface datasets\n",
        "dataset = load_dataset('csv', skiprows=1, data_files=data_paths, column_names = ['id', 'text', 'label'], delimiter='\\t')\n",
        "\n",
        "\n",
        "# Convert labels to one-hot encoding when task is multi-label classification\n",
        "def one_hot(example, num_labels=model_config[\"num_labels\"]):\n",
        "  new_label = []\n",
        "  for i in range(num_labels):\n",
        "    if str(i) in example[\"label\"].split(\",\"):\n",
        "      new_label.append(float(1))\n",
        "    else:\n",
        "      new_label.append(float(0))\n",
        "  example[\"label\"] = new_label\n",
        "  return example\n",
        "\n",
        "if task == \"multi_label_classification\":\n",
        "  dataset = dataset.map(one_hot)\n",
        "\n",
        "\n",
        "# Function for encoding (tokenizing) data\n",
        "def encode_data(data):\n",
        "  text = data[\"text\"]\n",
        "  label = data[\"label\"]\n",
        "\n",
        "  encoded_input = tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length= model_config[\"max_length\"],\n",
        "                padding= \"max_length\",\n",
        "                return_overflowing_tokens=True,\n",
        "                truncation=True\n",
        "            )\n",
        "  encoded_input[\"labels\"] = label\n",
        "  return encoded_input\n",
        "\n",
        "# Encode full dataset using map() function\n",
        "encoded_dataset = dataset.map(encode_data, batched=True)\n",
        "\n",
        "# Make PyTorch tensors\n",
        "encoded_dataset.set_format(\"torch\")\n",
        "\n",
        "\n",
        "## Choose an evaluation metric ##\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "\n",
        "  if task == \"single_label_classification\": # single-label classification\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    metric = accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  elif task == \"multi_label_classification\": # multi-label classification\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= 0.5)] = 1\n",
        "    metric = {'f1': f1_score(y_true=labels, y_pred=y_pred, average='micro')}\n",
        "\n",
        "  elif task == \"regression\":\n",
        "    metric = {'mse': mean_squared_error(labels, predictions, squared=False)}\n",
        "\n",
        "  return metric\n",
        "\n",
        "\n",
        "\n",
        "## Choose model ##\n",
        "\n",
        "if task != \"regression\":\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_config[\"model_weights\"],\n",
        "  problem_type=task, num_labels=model_config[\"num_labels\"], id2label=id2label, label2id=label2id)\n",
        "else:\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_config[\"model_weights\"],\n",
        "  problem_type=task, num_labels=model_config[\"num_labels\"])\n",
        "\n",
        "\n",
        "\n",
        "## Train and evaluate model ##\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"eval\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "if mode == \"train\":\n",
        "  trainer.train()\n",
        "  trainer.evaluate()\n",
        "\n",
        "\n",
        "\n",
        "## Inference ##\n",
        "\n",
        "if mode == \"inference\":\n",
        "  tokenizer = AutoTokenizer.from_pretrained(inference_modelpath)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(inference_modelpath)\n",
        "\n",
        "  if inference_mode == \"sentence\":\n",
        "    text = inference_input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad(): # run model\n",
        "      logits = model(**inputs).logits\n",
        "\n",
        "      if task == \"single_label_classification\":\n",
        "        predicted_class_id = logits.argmax().item()\n",
        "        print(model.config.id2label[predicted_class_id])\n",
        "\n",
        "      elif task == \"multi_label_classification\":\n",
        "        # apply sigmoid + threshold of 0.5\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(logits.squeeze().cpu())\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        # turn predicted id's into actual label names\n",
        "        predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "        print(predicted_labels)\n",
        "\n",
        "      elif task == \"regression\":\n",
        "        print(logits.squeeze().detach().numpy())\n",
        "\n",
        "\n",
        "  if inference_mode == \"dataset\":\n",
        "    # test arguments for Trainer\n",
        "    test_args = TrainingArguments(\n",
        "        output_dir = training_args.output_dir,\n",
        "        do_train = False,\n",
        "        do_predict = True,\n",
        "        per_device_eval_batch_size = training_args.per_device_train_batch_size,\n",
        "        dataloader_drop_last = False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "                  model = model,\n",
        "                  args = test_args,\n",
        "                  compute_metrics = compute_metrics)\n",
        "\n",
        "\n",
        "    # Run trainer in prediction mode\n",
        "    encoded_dataset[\"inference\"] = encoded_dataset[\"inference\"].remove_columns(\"label\")\n",
        "    prediction_output = trainer.predict(encoded_dataset[\"inference\"])\n",
        "    predictions = prediction_output[0]\n",
        "\n",
        "    ids = dataset[\"inference\"][\"id\"]\n",
        "    texts = dataset[\"inference\"][\"text\"]\n",
        "\n",
        "    if task == \"single_label_classification\":\n",
        "      preds = np.argmax(predictions, axis=1)\n",
        "      preds = [model.config.id2label[pred] for pred in preds]\n",
        "    elif task == \"multi_label_classification\":\n",
        "      sigmoid = torch.nn.Sigmoid()\n",
        "      probs = sigmoid(torch.Tensor(predictions))\n",
        "      preds = np.zeros(probs.shape)\n",
        "      preds[np.where(probs >= 0.5)] = 1\n",
        "      preds = [[id2label[idx] for idx, label in enumerate(pred) if label == 1.0] for pred in preds]\n",
        "    elif task == \"regression\":\n",
        "      preds = [float(pred[0]) for pred in predictions]\n",
        "\n",
        "    predictions_content = list(zip(ids, texts, preds))\n",
        "\n",
        "    # write predictions to file\n",
        "    f = open(pred_path, 'w')\n",
        "    f.write(\"id\\ttext\\tprediction\\n\")\n",
        "    for line in predictions_content:\n",
        "      if task == \"multi_label_classification\":\n",
        "        label = ','.join(line[2])\n",
        "        f.write(str(line[0]) + '\\t' + str(line[1]) + '\\t' + label + '\\n')\n",
        "      else:\n",
        "        f.write(str(line[0]) + '\\t' + str(line[1]) + '\\t' + str(line[2]) + '\\n')\n",
        "    f.close()\n",
        "    print(\"\\nPredictions saved in \" + pred_path)\n",
        "\n",
        "\n",
        "## Trial ##\n",
        "# Check whether everything works\n",
        "if mode == \"trial\":\n",
        "  print(dataset[\"train\"][\"label\"])\n"
      ],
      "metadata": {
        "id": "w1qVszJgVPh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}